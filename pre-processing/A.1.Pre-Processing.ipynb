{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M68Dz-4BLEw6"
   },
   "source": [
    "# MSCA 32018 NLP Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstructured Text Analysis for Anticipating AI shift in Job Industry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GARIMA SOHI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project focused on the future impact of AI on various industries and job lines by mining insights from a corpus of approximately 200K news articles about Data Science, Machine Learning, and Artificial Intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project is mainly divided into following modules:\n",
    "\n",
    "A. Pre-Processing\n",
    "B. Topic Modelling\n",
    "C. Sentiment Analysis - Named Entity Recognition \n",
    "D. Targeted Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Pre-Processing : PART-I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module focuses on following functions:\n",
    "1. Text Cleaning\n",
    "2. Lemmatization\n",
    "3. Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1591,
     "status": "ok",
     "timestamp": 1684369529481,
     "user": {
      "displayName": "Garima Sohi",
      "userId": "01650732343372855639"
     },
     "user_tz": 300
    },
    "id": "88wuS1PoLEw-",
    "outputId": "7daf94cf-6187-47db-82e6-0dac89e0f4bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 18:06:53.311198: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-27 18:06:56.295443: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-27 18:06:56.295582: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-27 18:06:56.295594: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-05-27 18:06:58.714827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-27 18:06:58.718879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-27 18:06:58.723496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-27 18:06:58.725742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-27 18:06:58.728296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-27 18:06:58.731456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries/packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import string\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27395,
     "status": "ok",
     "timestamp": 1684369556870,
     "user": {
      "displayName": "Garima Sohi",
      "userId": "01650732343372855639"
     },
     "user_tz": 300
    },
    "id": "0GEzkRL0LExB",
    "outputId": "38ca98d2-cb44-4c7c-c0b3-612bb7416609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200332, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "%time\n",
    "\n",
    "df_news_final_project = pd.read_parquet('https://storage.googleapis.com/msca-bdp-data-open/news_final_project/news_final_project.parquet', engine='pyarrow')\n",
    "\n",
    "# Check the shape of dataset\n",
    "df_news_final_project.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given dataset has ~200k news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1684369556871,
     "user": {
      "displayName": "Garima Sohi",
      "userId": "01650732343372855639"
     },
     "user_tz": 300
    },
    "id": "OCX8KPiKLIzI",
    "outputId": "2e3427b2-13b7-4802-e404-46b6e8cac870"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://en.people.cn/n3/2021/0318/c90000-983012...</td>\n",
       "      <td>2021-03-18</td>\n",
       "      <td>en</td>\n",
       "      <td>Artificial intelligence improves parking effic...</td>\n",
       "      <td>\\n\\nArtificial intelligence improves parking e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://newsparliament.com/2020/02/27/children-...</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>en</td>\n",
       "      <td>Children With Autism Saw Their Learning and So...</td>\n",
       "      <td>\\nChildren With Autism Saw Their Learning and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.dataweek.co.za/12835r</td>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>en</td>\n",
       "      <td>Forget ML, AI and Industry 4.0 – obsolescence ...</td>\n",
       "      <td>\\n\\nForget ML, AI and Industry 4.0 – obsolesce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url        date language   \n",
       "0  http://en.people.cn/n3/2021/0318/c90000-983012...  2021-03-18       en  \\\n",
       "1  http://newsparliament.com/2020/02/27/children-...  2020-02-27       en   \n",
       "2                   http://www.dataweek.co.za/12835r  2021-03-26       en   \n",
       "\n",
       "                                               title   \n",
       "0  Artificial intelligence improves parking effic...  \\\n",
       "1  Children With Autism Saw Their Learning and So...   \n",
       "2  Forget ML, AI and Industry 4.0 – obsolescence ...   \n",
       "\n",
       "                                                text  \n",
       "0  \\n\\nArtificial intelligence improves parking e...  \n",
       "1  \\nChildren With Autism Saw Their Learning and ...  \n",
       "2  \\n\\nForget ML, AI and Industry 4.0 – obsolesce...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the df_news_final_project DataFrame\n",
    "df_news_final_project.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1684369556975,
     "user": {
      "displayName": "Garima Sohi",
      "userId": "01650732343372855639"
     },
     "user_tz": 300
    },
    "id": "4pEiOl3WLaC4",
    "outputId": "5fc19ae0-79a6-4db7-daeb-98972397e194"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200332, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the DataFrame to include only English news articles\n",
    "df_news_en = df_news_final_project[df_news_final_project['language'] == 'en']\n",
    "\n",
    "# Get the shape of the filtered DataFrame\n",
    "df_news_en.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1684369557118,
     "user": {
      "displayName": "Garima Sohi",
      "userId": "01650732343372855639"
     },
     "user_tz": 300
    },
    "id": "iaH24Im6Lr_W",
    "outputId": "afa6b8ab-72b5-4f39-9ff2-5558fd9949c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url         0\n",
       "date        0\n",
       "language    0\n",
       "title       0\n",
       "text        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "missing_values = df_news_en.isna().sum()\n",
    "\n",
    "# Display the count of missing values for each column\n",
    "missing_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WW6LhbVKMSf4"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function to clean the text of news articles\n",
    "def text_cleaning(text, max_word_length=15):\n",
    "    \n",
    "    # Convert non-string inputs to string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters like '&nbsp;' and '\\xa0'\n",
    "    text = re.sub(r'&\\S*;|\\xa0', ' ', text)\n",
    "\n",
    "    # Remove any non-alphabetic character from the text\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    # text = text.lower()\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords, punctuation, and long words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation and len(word) <= max_word_length]\n",
    "    \n",
    "    # Remove leading/trailing white space\n",
    "    cleaned_text = ' '.join(tokens).strip()\n",
    "    \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply lemmatization with POS tagging to each word\n",
    "def lemmatize_with_pos(word):\n",
    "    pos = get_wordnet_pos(word)\n",
    "    if pos:\n",
    "        return lemmatizer.lemmatize(word, pos=pos)\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word)\n",
    "\n",
    "# Function to get the appropriate POS tag for a word\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # default to noun if not found\n",
    "\n",
    "# Function to apply lemmatization to each word\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatize_with_pos(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove duplicates from news articles\n",
    "def remove_duplicates(df):\n",
    "    \n",
    "    # Remove duplicate rows based on 'text', 'title', and 'date' columns\n",
    "    unique_df = df.drop_duplicates(subset=['text', 'title', 'date'])\n",
    "    \n",
    "    # Return the DataFrame with duplicate rows removed\n",
    "    return unique_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BU8b2YzoP7KV"
   },
   "outputs": [],
   "source": [
    "# Function to implement pre-processing\n",
    "def process_df(df):\n",
    "\n",
    "    # 1. Apply the text_cleaning function to the 'text' column using apply() method\n",
    "    print(\"Cleaning News Articles\")\n",
    "    start_time = time.time()\n",
    "    df['cleaned_text'] = df['text'].apply(text_cleaning)\n",
    "\n",
    "    cleaning_time = (time.time() - start_time) / 60\n",
    "    print(f\"Cleaning time: {cleaning_time:.2f} minutes\")\n",
    "    \n",
    "    # 2. Apply the lemmatization function to the 'text' column using apply() method\n",
    "    print(\"Lemmatizing cleaned News Articles\")\n",
    "    start_time = time.time()\n",
    "    df['lemmatized_text'] = df['cleaned_text'].apply(text_cleaning)\n",
    "\n",
    "    lemmatizing_time = (time.time() - start_time) / 60\n",
    "    print(f\"Lemmatization time: {lemmatizing_time:.2f} minutes\")\n",
    "    \n",
    "    # 3. Apply remove_duplicates function \n",
    "    print(\"Removing Duplicate Records\")\n",
    "    start_time = time.time()\n",
    "    df = remove_duplicates(df)\n",
    "    print('Number of unique News Articles after removing duplicates:', df.shape[0])\n",
    "\n",
    "    removal_time = (time.time() - start_time) / 60\n",
    "    print(f\"Removing duplicates time: {removal_time:.2f} minutes\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "2S7uqaGPQ0He",
    "outputId": "ee6cacb2-ca3f-4540-d055-476dbda54b4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning News Articles\n",
      "Cleaning time: 18.53 minutes\n",
      "Lemmatizing cleaned News Articles\n",
      "Lemmatization time: 11.85 minutes\n",
      "Removing Duplicate Records\n",
      "Number of unique News Articles after removing duplicates: 198735\n",
      "Removing duplicates time: 0.11 minutes\n"
     ]
    }
   ],
   "source": [
    "df_news_clean = process_df(df_news_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5Ebze_h2C5s1",
    "outputId": "cbdb4426-16ab-492c-a078-8526f9cf6ef9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://en.people.cn/n3/2021/0318/c90000-983012...</td>\n",
       "      <td>2021-03-18</td>\n",
       "      <td>en</td>\n",
       "      <td>Artificial intelligence improves parking effic...</td>\n",
       "      <td>\\n\\nArtificial intelligence improves parking e...</td>\n",
       "      <td>Artificial intelligence improves parking effic...</td>\n",
       "      <td>Artificial intelligence improves parking effic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://newsparliament.com/2020/02/27/children-...</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>en</td>\n",
       "      <td>Children With Autism Saw Their Learning and So...</td>\n",
       "      <td>\\nChildren With Autism Saw Their Learning and ...</td>\n",
       "      <td>Children With Autism Saw Their Learning Social...</td>\n",
       "      <td>Children With Autism Saw Their Learning Social...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.dataweek.co.za/12835r</td>\n",
       "      <td>2021-03-26</td>\n",
       "      <td>en</td>\n",
       "      <td>Forget ML, AI and Industry 4.0 – obsolescence ...</td>\n",
       "      <td>\\n\\nForget ML, AI and Industry 4.0 – obsolesce...</td>\n",
       "      <td>Forget ML AI Industry obsolescence focus Febru...</td>\n",
       "      <td>Forget ML AI Industry obsolescence focus Febru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url        date language   \n",
       "0  http://en.people.cn/n3/2021/0318/c90000-983012...  2021-03-18       en  \\\n",
       "1  http://newsparliament.com/2020/02/27/children-...  2020-02-27       en   \n",
       "2                   http://www.dataweek.co.za/12835r  2021-03-26       en   \n",
       "\n",
       "                                               title   \n",
       "0  Artificial intelligence improves parking effic...  \\\n",
       "1  Children With Autism Saw Their Learning and So...   \n",
       "2  Forget ML, AI and Industry 4.0 – obsolescence ...   \n",
       "\n",
       "                                                text   \n",
       "0  \\n\\nArtificial intelligence improves parking e...  \\\n",
       "1  \\nChildren With Autism Saw Their Learning and ...   \n",
       "2  \\n\\nForget ML, AI and Industry 4.0 – obsolesce...   \n",
       "\n",
       "                                        cleaned_text   \n",
       "0  Artificial intelligence improves parking effic...  \\\n",
       "1  Children With Autism Saw Their Learning Social...   \n",
       "2  Forget ML AI Industry obsolescence focus Febru...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  Artificial intelligence improves parking effic...  \n",
       "1  Children With Autism Saw Their Learning Social...  \n",
       "2  Forget ML AI Industry obsolescence focus Febru...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the resulting dataframe\n",
    "df_news_clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198735, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of dataset\n",
    "df_news_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of news articles reduced by ~2K after removing duplicates where date, title and text matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Scj0TrFv9AQh"
   },
   "outputs": [],
   "source": [
    "# Save DataFrame as Parquet\n",
    "df_news_clean.to_parquet('cleaned_news_articles.parquet', engine='pyarrow')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
